import tensorflow as tf
from tensorflow import keras
from tqdm import tqdm
import numpy as np
import keras.backend as K
import matplotlib.pyplot as plt

tf.random.set_seed(2023)


def process(img, labels):
	img = keras.applications.resnet.preprocess_input(img)
	return img, img


class Sample(keras.Model):
	def __init__(self):
		super(Sample, self).__init__()
		self.k = keras.layers.Conv2D(filters=231, kernel_size=16, strides=16, use_bias=False)
		self.k_auxiliary = keras.layers.Conv2DTranspose(filters=3, kernel_size=16, strides=16, use_bias=False)

	def call(self, x):
		out = self.k(x)
		out = self.k_auxiliary(out)
		return out


@tf.function
def train_step(imgs):
	with tf.GradientTape() as tape:
		out = model(imgs)
		loss = tf.reduce_mean(loss_fn(out, imgs))
	gradient = tape.gradient(loss, model.trainable_variables)
	optimizer.apply_gradients(zip(gradient, model.trainable_variables))
	return loss



if __name__ == "__main__":
	dataset = keras.utils.image_dataset_from_directory("data", label_mode="categorical", batch_size=64,
													   image_size=(224, 224))
	# 定义划分比例
	train_ratio = 0.8  # 训练集比例
	val_ratio = 0.1  # 验证集比例
	test_ratio = 0.1  # 测试集比例
	dataset_size = len(dataset)
	dataset = dataset.shuffle(dataset_size)
	# 计算划分后的样本数量
	train_size = int(train_ratio * dataset_size)
	val_size = int(val_ratio * dataset_size)
	test_size = int(test_ratio * dataset_size)

	# 划分数据集
	train_dataset = dataset.take(train_size)
	val_dataset = dataset.skip(train_size).take(val_size)
	test_dataset = dataset.skip(train_size + val_size).take(test_size)

	train_dataset = train_dataset.map(process).prefetch(tf.data.experimental.AUTOTUNE)
	val_dataset = val_dataset.map(process).prefetch(tf.data.experimental.AUTOTUNE)
	test_dataset = test_dataset.map(process).prefetch(tf.data.experimental.AUTOTUNE)
	optimizer = keras.optimizers.Adam(1e-3)
	loss_fn = keras.losses.MeanAbsoluteError()
	model = Sample()
	for epoch in range(20):
		epoch_loss = []
		for imgs, _ in train_dataset:
			batch_loss = train_step(imgs)
			epoch_loss.append(batch_loss.numpy())
		print(np.mean(epoch_loss))
		val_dataset = iter(val_dataset)
		imgs, _ = next(val_dataset)
		out = model(imgs)
		mean = K.constant(np.array([103.939, 116.779, 123.68]))
		img = K.bias_add(out, mean)
		img = (img[..., ::-1][0] / 255).numpy()
		img = np.clip(img, 0, 1)
		# RGB-->BGR
		plt.imsave("rec.png", img)
	model.save_weights("sample.h5")

